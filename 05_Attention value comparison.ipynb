import pandas as pd
import numpy as np
import gc
import random
from random import sample
import pickle
import time
import umap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import pairwise_distances
from transformers import BertForMaskedLM, BertTokenizer, BertForSequenceClassification, BertModel
from bertviz import head_view, model_view
import spacy

%matplotlib inline

def tensor_to_df(att_values, classes, layer=1, head=1, max_heads=12, max_layers=12):
    """
    This function reduces an attention value tensor to a 2D DataFrame containing one value per attention head per layer.
    First it creates a vector containing the first layer's attention heads.
    Afterwards the code loops through the remaining layers and concatenates the resulting vectors.  
    """

    att_values = att_values[classes]
    
    # Define the UMAP reducer
    reducer = umap.UMAP(n_neighbors = 5,       # balance between local and global structures
                        min_dist = 0.1,        # minimum distance between points allowed
                        n_components = 1,      # number of dimensions to be reduced to
                        metric = 'euclidean',  # how to compute distance
                        random_state=42)
    
    #print('    DONE.')
    print('Start extraction and transformation...')
    start = time.time()

    # Extract the first attention head of the first layer and reduce it 
    att_head = pd.DataFrame([attention.detach().numpy() for attention in att_values[layer-1][0][head-1]])
    embed_head = reducer.fit_transform(att_head)
    head_df = pd.DataFrame(data=embed_head, columns=[head])

    # Do the same with the remaining attention heads in the first layer
    for h in range(head, max_heads):
        att_head = pd.DataFrame([attention.detach().numpy() for attention in att_values[layer-1][0][h]])
        embed_head = reducer.fit_transform(att_head)
        next_head = pd.DataFrame(data=embed_head, columns=[h+1])
        head_df = pd.concat([head_df, next_head] , axis=1)

    # Transpose the df to reduce the other dimension of the heads and create the first layer vector
    embed_layer = reducer.fit_transform(head_df.transpose())
    layer_df = pd.DataFrame(data=embed_layer, columns=['Layer '+str(layer)])

    # Loop through the remaining layers
    for l in range(layer, max_layers):
    
        # Extract the first attention head of the next layer and reduce it 
        att_head = pd.DataFrame([attention.detach().numpy() for attention in att_values[l][0][head-1]])
        embed_head = reducer.fit_transform(att_head)
        head_df = pd.DataFrame(data=embed_head, columns=[head])
    
        # Do the same with the remaining attention heads in the next layer
        for h in range(head, max_heads):
            att_head = pd.DataFrame([attention.detach().numpy() for attention in att_values[l][0][h]])
            embed_head = reducer.fit_transform(att_head)
            next_head = pd.DataFrame(data=embed_head, columns=[h+1])
            head_df = pd.concat([head_df, next_head] , axis=1)
    
        # Transpose the df to reduce the other dimension of the heads and create the next layer vector
        embed_layer = reducer.fit_transform(head_df.transpose())
        next_layer = pd.DataFrame(data=embed_layer, columns=['Layer '+str(l+1)])
        
        # Concatenate all vectors to one df
        layer_df = pd.concat([layer_df, next_layer] , axis=1)
    
    layer_df['classes'] = [classes for x in range(len(layer_df))]
    layer_df['Head'] = list(layer_df.index+1)
    
    end = time.time()
    print('    DONE.')
    print('Reducing the attention values took', (end - start)/60, 'min.\n')
    
    return layer_df

def calc_distance(metric, classes, X, Y, Z):
    """
    Calculate a specified distance measure between the vectores of layers 
    of predefined DataFrames and a specific class.
    """
        
    X = X[X['classes'] == classes]
    Y = Y[Y['classes'] == classes]
    Z = Z[Z['classes'] == classes]
    sim_1 = []
    for i in range(12):
        sim = pairwise_distances(X=np.array(X.iloc[:,i]).reshape(1, -1), Y=np.array(Y.iloc[:,i]).reshape(1, -1), metric=metric)
        sim_1.append(sim[0][0])
    
    sim_2 = []
    for i in range(12):
        sim = pairwise_distances(X=np.array(X.iloc[:,i]).reshape(1, -1), Y=np.array(Z.iloc[:,i]).reshape(1, -1), metric=metric)
        sim_2.append(sim[0][0])
    
    sim_3 = []
    for i in range(12):
        sim = pairwise_distances(X=np.array(Y.iloc[:,i]).reshape(1, -1), Y=np.array(Z.iloc[:,i]).reshape(1, -1), metric=metric)
        sim_3.append(sim[0][0])
    
    li = [sim_1, sim_2, sim_3]
    df = pd.DataFrame(data = np.array(li).transpose(),
                      index = ['1','2','3','4','5','6','7','8','9','10','11','12'],
                      columns = ['Bertbase_pretrained', 'Bertbase_Bertfine', 'pretrained_Bertfine'])
    return df

# Plot the resulting embedding for all 12 layers

def plot_attention(classes, df):
    """
    Slice the a DataFrame containing attention values according to a class
    and plot these values per head per layer and color coded on the model.
    """
    
    print('UMAP reduced Layers on', classes, 'class:\n\n',
          'Legend: Blue:\t Bertbase\n',
              '\t Orange: pretrained\n',
              '\t Green:\t Bertfine')
    
    warnings.filterwarnings("ignore")
    
    attentions = df[df["classes"] == classes]
    
    fig = plt.figure(figsize=(18,20))
    
    for layer in range(12):
        axes = plt.subplot(4,3,layer+1)
        axes.scatter(attentions.iloc[:,-2],
                     attentions.iloc[:,layer],
                     c=[sns.color_palette()[x] for x in attentions.Model.map({'Bertbase':0, 'pretrained':1, 'Bertfine':2})])
        axes = plt.title("Layer "+str(layer+1))
        axes = plt.xlabel("heads")
        axes = plt.ylabel("UMAP reduced attention values")
        plt.gca().set_aspect('equal', 'datalim')
#--> 0=blue, 1=orange, 2=green 

# Import attention values

# Load all attention values
attentions_Bertbase = pickle.load(open("attention_values/attentions_Bertbase.pkl", "rb"))
attentions_pretrained = pickle.load(open("attention_values/attentions_pretrained.pkl", "rb"))
attentions_Bertfine = pickle.load(open("attention_values/attentions_Bertfine.pkl", "rb"))

# Check on the shape of all attention tensors
print("Bertbase:")
for i in range(12):
    print("Layer",i+1,":",attentions_Bertbase['0'][i].size())

print("\npretrained:")
for i in range(12):
    print("Layer",i+1,":",attentions_pretrained['negative'][i].size())

print("\nBertfine:")
for i in range(12):
    print("Layer",i+1,":",attentions_Bertfine['0'][i].size())
# [batch_size, num_heads, sequence_length, sequence_length]

# Transform and reduce attention tensors

# Transform tensors to dfs and add class marker and head number

Bertbase_neg = tensor_to_df(att_values=attentions_Bertbase, classes='0')
Bertbase_pos = tensor_to_df(att_values=attentions_Bertbase, classes='2')
Bertbase_neu = tensor_to_df(attentions_Bertbase, classes='1')
print('\n1/3\n')

pretrained_neg = tensor_to_df(attentions_pretrained, classes='negative')
pretrained_pos = tensor_to_df(attentions_pretrained, classes='positive')
pretrained_neu = tensor_to_df(attentions_pretrained, classes='neutral')
print('\n2/3\n')

Bertfine_neg = tensor_to_df(attentions_Bertfine, classes='0')
Bertfine_pos = tensor_to_df(attentions_Bertfine, classes='2')
Bertfine_neu = tensor_to_df(attentions_Bertfine, classes='1')


# Concat all dfs per model and add a model marker

Bertbase_df = pd.concat([Bertbase_pos, Bertbase_neu, Bertbase_neg] , axis=0)
Bertbase_df['Model'] = ['Bertbase' for x in range(len(Bertbase_df))]
pretrained_df = pd.concat([pretrained_pos, pretrained_neu, pretrained_neg] , axis=0)
pretrained_df['Model'] = ['pretrained' for x in range(len(pretrained_df))]
Bertfine_df = pd.concat([Bertfine_pos, Bertfine_neu, Bertfine_neg] , axis=0)
Bertfine_df['Model'] = ['Bertfine' for x in range(len(Bertfine_df))]

# Reverse the encoding of classes
encode_classes_dict = {
    '2' : 'positive',
    '1' : 'neutral',
    '0' : 'negative'}

Bertfine_df['classes'] = [encode_classes_dict[x] for x in Bertfine_df.classes]
Bertbase_df['classes'] = [encode_classes_dict[x] for x in Bertbase_df.classes]

# Save the dataframes as pickle files for not re-doing the tensor_to_df
pickle.dump(Bertbase_df, open('attention_values/Bertbase_df.pkl', 'wb'))
pickle.dump(pretrained_df, open('attention_values/pretrained_df.pkl', 'wb'))
pickle.dump(Bertfine_df, open('attention_values/Bertfine_df.pkl', 'wb'))

gc.collect()

# Model-wise inspection

# Read in processed attention dfs
Bertbase_df = pickle.load(open("attention_values/Bertbase_df.pkl", "rb"))
pretrained_df = pickle.load(open("attention_values/pretrained_df.pkl", "rb"))
Bertfine_df = pickle.load(open("attention_values/Bertfine_df.pkl", "rb"))

## Calculate distance between layers

# Calculate cosine distance for classes
negative_cos_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='cosine', classes='negative')
positive_cos_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='cosine', classes='positive')
neutral_cos_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='cosine', classes='neutral')

# Calculate euclidean distance for classes
negative_euc_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='euclidean', classes='negative')
positive_euc_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='euclidean', classes='positive')
neutral_euc_df = calc_distance(X=Bertbase_df, Y=pretrained_df, Z=Bertfine_df, metric='euclidean', classes='neutral')

# --> look for values that are zero in the euclidean version but non-zeros in the cosine version and vice-versa

positive_cos_df
# Which model is further from Base?
# Layers 1    : same
# Layers 2    : pretrained
# Layer 3-6   : Bertfine
# Layer 7-8   : pretrained
# Layers 9-11 : Bertfine
# Layer 12    : pretrained

# --> Bertfine is more often further away

positive_euc_df
# Which model is further from Base?
# Layers 1    : Bertfine 
# Layers 2    : pretrained 
# Layers 3-6  : Bertfine 
# Layers 7-8  : pretrained
# Layers 9-11 : Bertfine 
# Layers 12   : pretrained

# --> Bertfine is more often further away

## Plotting attention values per layer

attentions_df = pd.concat([Bertbase_df, pretrained_df, Bertfine_df])

# Plot attention values of positive class all heads for each layer
plot_attention(classes='positive', df=attentions_df)

# Plot attention values of negative class all heads for each layer
plot_attention(classes='negative', df=attentions_df)

# Plot attention values of neutral class all heads for each layer
plot_attention(classes='neutral', df=attentions_df)

# Word-wise inspection

## BertViz

def show_head_view(model, tokenizer, sentence_a, sentence_b=None, layer=None, heads=None):
    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)
    input_ids = inputs['input_ids']
    if sentence_b:
        token_type_ids = inputs['token_type_ids']
        attention = model(input_ids, token_type_ids=token_type_ids)[-1]
        sentence_b_start = token_type_ids[0].tolist().index(1)
    else:
        attention = model(input_ids)[-1]
        sentence_b_start = None
    input_id_list = input_ids[0].tolist() # Batch index 0
    tokens = tokenizer.convert_ids_to_tokens(input_id_list)    
    head_view(attention, tokens, sentence_b_start, layer=layer, heads=heads)

def show_model_view(model, tokenizer, sentence_a, sentence_b=None, hide_delimiter_attn=False, display_mode="dark"):
    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)
    input_ids = inputs['input_ids']
    if sentence_b:
        token_type_ids = inputs['token_type_ids']
        attention = model(input_ids, token_type_ids=token_type_ids)[-1]
        sentence_b_start = token_type_ids[0].tolist().index(1)
    else:
        attention = model(input_ids)[-1]
        sentence_b_start = None
    input_id_list = input_ids[0].tolist() # Batch index 0
    tokens = tokenizer.convert_ids_to_tokens(input_id_list)  
    if hide_delimiter_attn:
        for i, t in enumerate(tokens):
            if t in ("[SEP]", "[CLS]"):
                for layer_attn in attention:
                    layer_attn[0, :, i, :] = 0
                    layer_attn[0, :, :, i] = 0
    model_view(attention, tokens, sentence_b_start, display_mode=display_mode)

### Load data and define test sentences

# Load review data 
preprocessed_text = pickle.load(open("data/preprocessed_text.pkl", "rb"))

# Create known Train and Test split
reviews_train, reviews_test, y_train, y_test = train_test_split(preprocessed_text.text, preprocessed_text.classes,
                                                                test_size = 0.2, random_state = 42,
                                                                stratify=preprocessed_text.classes)

# Find the shortest reviews in the test set that contain the words great and bad

great = preprocessed_text[preprocessed_text.text.str.contains('great')]
great_73 = great[great['text length'] <= 73]
great_idx = great_73.index
great_test = reviews_test[reviews_test.index.isin(great_idx)]

bad = preprocessed_text[preprocessed_text.text.str.contains('bad')]
bad_74 = bad[bad['text length'] <= 74]
bad_idx = bad_74.index
bad_test = reviews_test[reviews_test.index.isin(bad_idx)]

# Assign these two of the reviews to the sentences to be inspected by BertViz
sentence_a = great_test.iloc[0]
sentence_b = bad_test.iloc[1]

# Number of reviews contained
len(great_test)
#len(bad_test)

### Bert base model

# Get model view for pretrained Model
 
model_version = 'bert-base-uncased'
do_lower_case = True
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode="dark")

# Get head view for Base Model

model_version = 'bert-base-uncased'
do_lower_case = True
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_head_view(model, tokenizer, sentence_a)

### Pre-trained model

# Get model view for pretrained Model

model_version = 'output_pretrained/'
do_lower_case = True
model = BertForMaskedLM.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode="dark")

# Get head view for pretrained Model

model_version = 'output_pretrained/'
do_lower_case = True
model = BertForMaskedLM.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_head_view(model, tokenizer, sentence_a)

### Fine-tuned model

# Get model view for finetuned Model

model_version = 'output_finetuned/'
do_lower_case = True
model = BertForSequenceClassification.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode="dark")

# Get attention values for finetuned Model

model_version = 'output_finetuned/'
do_lower_case = True
model = BertForSequenceClassification.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
show_head_view(model, tokenizer, sentence_a)

## Manual retrieval of attention values

# Define a word and get its attention values to all other tokens in the sentence
def per_word_att(word, sentences, model_origin, model_type):
    
    """
    This function retrieves attention values of a defined BERT model.
    Attention values are retrieved for a specific key word within a certain set of sentences.
    The attention values of a word are summed up over all heads in a layer.
    The output is a dictionary of dataframes containing the relationship of the key word to all other tokens per layer.
    """
    # Load model and tokenizer
    model = model_type.from_pretrained(model_origin, output_attentions=True)
    tokenizer = BertTokenizer.from_pretrained(model_origin, never_split=[word], do_lower_case=True)
    dic = {}
    
    for i in range(len(sentences)):
        sentence = sentences.iloc[i]
        
        # tokenize the text and retrieve the attentions
        inputs = tokenizer.encode_plus(sentence,                 # Sentence to encode.
                               add_special_tokens = True,        # Add '[CLS]' and '[SEP]'
                               padding = 'longest',              # Pad & truncate all sentences.
                               #max_length ='max',
                               truncation = True,                # truncate sample if too long
                               pad_to_max_length = True,         # add padding tokens if shorter sequence
                               return_attention_mask = True,     # Construct attn. masks.
                               return_tensors = 'pt')            # return pytorch tensors

        outputs = model(**inputs)
        
        # Check if word is truncated during tokenization. If is go to next sentence, otherwise get attention values.
        l = tokenizer.convert_ids_to_tokens(inputs["input_ids"].numpy()[0])
        if word not in l:
            continue
        
        idx = tokenizer.convert_ids_to_tokens(inputs["input_ids"].numpy()[0]).index(word)     # get index of the key word in the tokens
        cols = tokenizer.convert_ids_to_tokens(inputs["input_ids"].numpy()[0])                # assign the tokens as the columns
        att_head = []
        att_word = []
        batch = 0
    
        for layer in range(12):
            for head in range(12):
            
                # retrieve attention for all heads in one layer for that exact word
                att_head.append(outputs['attentions'][layer][batch][head][idx][:].detach().numpy())  
        
            # Append all attentions layer by layer to an overall list 
            att_word.append(att_head)
            att_head = []

        sum_att = [sum(att) for att in att_word]      # add up the attentions for each word over all heads in each layer
        df = pd.DataFrame(sum_att, columns=cols)      # save the attentions of the key word to each other token per layer
        df['Layer'] = list(df.index+1)
        df['Model'] = [model_origin for x in range(len(df))]
        
        # add it to the dictionary, having one entry per layer
        dic["{0}".format(i)] = df
        
    return dic

gc.collect()

# Download spacy model for POS tagging
# python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm")

### Important words from supervised learning  also using POS (Group 1)

### GREAT

# get a random 100 reviews from the test set that contain the word 'great' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
great_idx = preprocessed_text[(preprocessed_text['text length'] <= 100)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('great'))].index # ... sometimes 'great' is cut off then
great_rev = reviews_test[reviews_test.index.isin(great_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(great_rev.index), 100)                                          # sample down to 100 reviews
great_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
#len(great_rev)

# Get attention values for great from test set sentences for each model
great_base_100 = per_word_att(word='great', sentences=great_test_100, model_origin='bert-base-uncased', model_type=BertModel)
great_pretrained_100 = per_word_att(word='great', sentences=great_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
great_fine_100 = per_word_att(word='great', sentences=great_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(great_base_100, open('attention_values/great_base_100.pkl', 'wb'))
pickle.dump(great_pretrained_100, open('attention_values/great_pretrained_100.pkl', 'wb'))
pickle.dump(great_fine_100, open('attention_values/great_fine_100.pkl', 'wb'))

# Read in retrieved attentions
great_base_100 = pickle.load(open("attention_values/great_base_100.pkl", "rb"))
great_pretrained_100 = pickle.load(open("attention_values/great_pretrained_100.pkl", "rb"))
great_fine_100 = pickle.load(open("attention_values/great_fine_100.pkl", "rb"))

#### GREAT's attention to all words

# Find Top 5 of all words GREAT is attending to

sentences = list(great_base_100.keys())
great_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([great_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    great_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    great_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    great_top = pd.concat([great_top, df], axis=1)

great_top = great_top.drop('great', axis='columns').transpose()
great_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(great_top[1]) / np.mean(great_top[0]) - 1, 4)
fine = np.round(np.mean(great_top[2]) / np.mean(great_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

great_top['word'] = great_top.index 
great_top = great_top.groupby(['word']).mean()

##### Top 5 words

great_top.sort_values(ascending=False, by=[0]).head(5)

great_top.sort_values(ascending=False, by=[1]).head(5)

great_top.sort_values(ascending=False, by=[2]).head(5)

sentences = list(great_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(great_base_100[sentence].columns))
matches = [match for match in l if "costello" in match] 
print(matches)

#### GREAT's attention to all nouns

# Attentions for word GREAT

sentences = list(great_base_100.keys())
great_noun = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([great_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    great_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    great_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "NOUN"] # get a list of nouns in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                              # filter the df on all adjectives
    great_noun = pd.concat([great_noun, df], axis=1)

# print statistics for the models
#great_noun.transpose().describe())
great_noun.transpose().describe()

##### Top 5 nouns

great_noun = great_noun.transpose()
great_noun['word'] = great_noun.index 

great_noun.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

great_noun.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

great_noun.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

# Why doesn't 'costello' appear in here?
doc = nlp('[CLS] costello great place grab burger couple beers friendly service casual atmosphere plenty room friends [SEP]')
# Token and Tag
for token in doc:
    print(token, token.pos_)
# --> it is classified as a verb not as a noun

#### GREAT's attention to all verbs

# Attentions for word GREAT

sentences = list(great_base_100.keys())
great_verb = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([great_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    great_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    great_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "VERB"] # get a list of nouns in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                              # filter the df on all adjectives
    great_verb = pd.concat([great_verb, df], axis=1)

# print statistics for the models
#great_verb.transpose().describe())

great_verb.transpose().describe()

###### Top 5 verbs

great_verb = great_verb.transpose()
great_verb['word'] = great_verb.index

great_verb.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

great_verb.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

great_verb.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

sentences = list(great_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(great_base_100[sentence].columns))
matches = [match for match in l if "recommendation" in match] 
print(matches)

# Why are 'recommendation', 'service' and 'waitress' in there?
doc = nlp('visiting chicago stopped yesterday based locals recommendation great coffee incredible service')
# Token and Tag
for token in doc:
    print(token, token.pos_)
# --> spaCy classifies them as verbs

#### GREAT's attention to all adjectives

# Attentions for word GREAT

sentences = list(great_base_100.keys())
great_adj = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([great_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    great_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    great_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "ADJ"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    great_adj = pd.concat([great_adj, df], axis=1)

# print statistics for the models
#print(great_adj.transpose().describe())
great_adj.drop('great', axis='columns').transpose().describe()

###### Top 5 adjectives

great_adj = great_adj.drop('great', axis='columns').transpose()
great_adj['word'] = great_adj.index 

great_adj.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

great_adj.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

great_adj.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

### DELICIOUS

# get a random 100 reviews from the test set that contain the word 'delicious' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
delicious_idx = preprocessed_text[(preprocessed_text['text length'] <= 200)                 # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('delicious'))].index   # ... sometimes keyword is cut off then
delicious_rev = reviews_test[reviews_test.index.isin(delicious_idx)]                        # get the indices of the ones that are in the test set
rnd_idx = sample(list(delicious_rev.index), min(len(delicious_rev), 100))                   # sample down to 100 reviews
delicious_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]  

# Get attention values for delicious from test set sentences for each model
delicious_base_100 = per_word_att(word='delicious', sentences=delicious_test_100, model_origin='bert-base-uncased', model_type=BertModel)
delicious_pretrained_100 = per_word_att(word='delicious', sentences=delicious_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
delicious_fine_100 = per_word_att(word='delicious', sentences=delicious_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(delicious_base_100, open('attention_values/delicious_base_100.pkl', 'wb'))
pickle.dump(delicious_pretrained_100, open('attention_values/delicious_pretrained_100.pkl', 'wb'))
pickle.dump(delicious_fine_100, open('attention_values/delicious_fine_100.pkl', 'wb'))

# Read in retrieved attentions
delicious_base_100 = pickle.load(open("attention_values/delicious_base_100.pkl", "rb"))
delicious_pretrained_100 = pickle.load(open("attention_values/delicious_pretrained_100.pkl", "rb"))
delicious_fine_100 = pickle.load(open("attention_values/delicious_fine_100.pkl", "rb"))

#### DELICIOUS' attention to all words

# Find Top 5 of all words DELICIOUS is attending to

sentences = list(delicious_base_100.keys())
delicious_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([delicious_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    delicious_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    delicious_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    delicious_top = pd.concat([delicious_top, df], axis=1)

delicious_top = delicious_top.drop('delicious', axis='columns').transpose()
delicious_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(delicious_top[1]) / np.mean(delicious_top[0]) - 1, 4)
fine = np.round(np.mean(delicious_top[2]) / np.mean(delicious_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for finetuning: {1}'.format(pre, fine))

delicious_top['word'] = delicious_top.index 
delicious_top = delicious_top.groupby(['word']).mean()

##### Top 5 words

delicious_top.sort_values(ascending=False, by=[0]).head(5)

delicious_top.sort_values(ascending=False, by=[1]).head(5)

# Why is ##ness rated so high?
sentences = list(delicious_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(delicious_base_100[sentence].columns))
matches = [match for match in l if "##ness" in match] 
print(matches)
# --> it probably belongs to delicious as a suffix

delicious_top.sort_values(ascending=False, by=[2]).head(5)

# Why is ##the rated so high?
sentences = list(delicious_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(delicious_base_100[sentence].columns))
matches = [match for match in l if "##the" in match] 
print(matches)
# --> it probably belongs to delicious as a suffix

#### DELICIOUS' attention to all nouns

# Attentions for word DELICIOUS

sentences = list(delicious_base_100.keys())
delicious_noun = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([delicious_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    delicious_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    delicious_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "NOUN"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    delicious_noun = pd.concat([delicious_noun, df], axis=1)

# print statistics for the models
#print(delicious_noun.transpose().describe())
delicious_noun.transpose().describe()

##### Top 5 nouns

delicious_noun = delicious_noun.transpose()
delicious_noun['word'] = delicious_noun.index 

delicious_noun.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

delicious_noun.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

delicious_noun.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

#### DELICIOUS' attention to all verbs

# Attentions for word DELICIOUS

sentences = list(delicious_base_100.keys())
delicious_verb = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([delicious_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    delicious_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    delicious_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "VERB"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    delicious_verb = pd.concat([delicious_verb, df], axis=1)

# print statistics for the models
#print(delicious_verb.transpose().describe())
delicious_verb.drop('delicious', axis='columns').transpose().describe()

###### Top 5 verbs

delicious_verb = delicious_verb.drop('delicious', axis='columns').transpose()
delicious_verb['word'] = delicious_verb.index 

delicious_verb.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

delicious_verb.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

delicious_verb.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

#### DELICIOUS' attention to all adjectives

# Attentions for word DELICIOUS

sentences = list(delicious_base_100.keys())
delicious_adj = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([delicious_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    delicious_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    delicious_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "ADJ"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    delicious_adj = pd.concat([delicious_adj, df], axis=1)

# print statistics for the models
#print(delicious_adj.transpose().describe())
delicious_adj.drop('delicious', axis='columns').transpose().describe()

##### Top 5 adjectives

delicious_adj = delicious_adj.drop('delicious', axis='columns').transpose()
delicious_adj['word'] = delicious_adj.index 

delicious_adj.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

delicious_adj.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

delicious_adj.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

### GOOD

# get a random 100 reviews from the test set that contain the word 'good' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
good_idx = preprocessed_text[(preprocessed_text['text length'] <= 100)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('good'))].index # ... sometimes keyword is cut off then
good_rev = reviews_test[reviews_test.index.isin(good_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(good_rev.index), 100)                                          # sample down to 100 reviews
good_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews

good_base_100 = per_word_att(word='good', sentences=good_test_100, model_origin='bert-base-uncased', model_type=BertModel)
good_pretrained_100 = per_word_att(word='good', sentences=good_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
good_fine_100 = per_word_att(word='good', sentences=good_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(good_base_100, open('attention_values/good_base_100.pkl', 'wb'))
pickle.dump(good_pretrained_100, open('attention_values/good_pretrained_100.pkl', 'wb'))
pickle.dump(good_fine_100, open('attention_values/good_fine_100.pkl', 'wb'))

# Read in retrieved attentions
good_base_100 = pickle.load(open("attention_values/good_base_100.pkl", "rb"))
good_pretrained_100 = pickle.load(open("attention_values/good_pretrained_100.pkl", "rb"))
good_fine_100 = pickle.load(open("attention_values/good_fine_100.pkl", "rb"))

#### GOOD's attention to all words

# Find Top 5 of all words GOOD is attending to

sentences = list(good_base_100.keys())
good_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([good_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    good_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    good_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    good_top = pd.concat([good_top, df], axis=1)
    
good_top = good_top.drop('good', axis='columns').transpose()
good_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(good_top[1]) / np.mean(good_top[0]) - 1, 4)
fine = np.round(np.mean(good_top[2]) / np.mean(good_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

good_top['word'] = good_top.index 
good_top = good_top.groupby(['word']).mean()

##### Top 5 words

good_top.sort_values(ascending=False, by=[0]).head(5)

good_top.sort_values(ascending=False, by=[1]).head(5)

good_top.sort_values(ascending=False, by=[2]).head(5)

#### GOOD's attention to all nouns

# Attentions for word GOOD

sentences = list(good_base_100.keys())
good_noun = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([good_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    good_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    good_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "NOUN"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    good_noun = pd.concat([good_noun, df], axis=1)

# print statistics for the models
#print(good_fil.transpose().describe())
good_noun.transpose().describe()

##### Top 5 nouns

good_noun = good_noun.transpose()
good_noun['word'] = good_noun.index 

good_noun.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

good_noun.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

good_noun.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

#### GOOD's attention to all verbs

# Attentions for word GOOD

sentences = list(good_base_100.keys())
good_verb = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([good_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    good_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    good_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "VERB"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    good_verb = pd.concat([good_verb, df], axis=1)

# print statistics for the models
#print(good_verb.transpose().describe())
good_verb.transpose().describe()

##### Top 5 verbs

good_verb = good_verb.transpose()
good_verb['word'] = good_verb.index 

good_verb.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

good_verb.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

good_verb.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

#### GOOD's attention to all adjectives

# Attentions for word GOOD

sentences = list(good_base_100.keys())
good_adj = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([good_base_100[sentence].iloc[:,0:-2].sum(axis=0),            # Base model is the first row  
                    good_pretrained_100[sentence].iloc[:,0:-2].sum(axis=0),      # Pretrained model is the second row
                    good_fine_100[sentence].iloc[:,0:-2].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    cols = df.columns[0:-2]                                              # get the tokens from the columns
    cols = ' '.join(cols)                                                # convert them to a string
    filters = [token.text for token in nlp(cols) if token.pos_ == "ADJ"] # get a list of adjectives in the sentence as filters
    df = df.loc[:, df.columns.isin(filters)]                         # filter the df on all adjectives
    good_adj = pd.concat([good_adj, df], axis=1)

# print statistics for the models
#print(good_fil.transpose().describe())
good_adj.drop('good', axis='columns').transpose().describe()

##### Top 5 adjectives

good_adj = good_adj.drop('good', axis='columns').transpose()
good_adj['word'] = good_adj.index 

good_adj.groupby(['word']).mean().sort_values(ascending=False, by=[0]).head(5)

good_adj.groupby(['word']).mean().sort_values(ascending=False, by=[1]).head(5)

good_adj.groupby(['word']).mean().sort_values(ascending=False, by=[2]).head(5)

### Unimportant words from supervised learning (Group 2)

### FOOD

# get a random 100 reviews from the test set that contain the word 'FOOD' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
food_idx = preprocessed_text[(preprocessed_text['text length'] <= 100)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('food'))].index # ... sometimes 'food' is cut off then
food_rev = reviews_test[reviews_test.index.isin(food_idx)]                            # get the indices of the ones that are in the test set
rnd_idx = sample(list(food_rev.index), min(len(food_rev), 100))                       # sample down to 100 reviews
food_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]  
len(food_rev)

# Get attentions for food for test set sentences from all models
food_base_100 = per_word_att(word='food', sentences=food_test_100, model_origin='bert-base-uncased', model_type=BertModel)
food_pretrained_100 = per_word_att(word='food', sentences=food_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
food_fine_100 = per_word_att(word='food', sentences=food_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(food_base_100, open('attention_values/food_base_100.pkl', 'wb'))
pickle.dump(food_pretrained_100, open('attention_values/food_pretrained_100.pkl', 'wb'))
pickle.dump(food_fine_100, open('attention_values/food_fine_100.pkl', 'wb'))

# Read in retrieved attentions
food_base_100 = pickle.load(open("attention_values/food_base_100.pkl", "rb"))
food_pretrained_100 = pickle.load(open("attention_values/food_pretrained_100.pkl", "rb"))
food_fine_100 = pickle.load(open("attention_values/food_fine_100.pkl", "rb"))

#### FOOD's attention to all words

# Find Top 5 of all words FOOD is attending to

sentences = list(food_base_100.keys())
food_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([food_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    food_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    food_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    food_top = pd.concat([food_top, df], axis=1)
    
food_top = food_top.drop('food', axis='columns').transpose()
food_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(food_top[1]) / np.mean(food_top[0]) - 1, 4)
fine = np.round(np.mean(food_top[2]) / np.mean(food_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

food_top['word'] = food_top.index 
food_top = food_top.groupby(['word']).mean()

food_top.sort_values(ascending=False, by=[0]).head(5)

food_top.sort_values(ascending=False, by=[1]).head(5)

food_top.sort_values(ascending=False, by=[2]).head(5)

# What is the context in the review containing 'poisoning'? 
sentences = list(food_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(food_base_100[sentence].columns))
matches = [match for match in l if "poisoning" in match] 
print(matches)
# --> hardly to tell the exact context, rather positive

### LEVEL

# get a random 100 reviews from the test set that contain the word 'level' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
level_idx = preprocessed_text[(preprocessed_text['text length'] <= 700)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('level'))].index # ... sometimes 'level' is cut off then
level_rev = reviews_test[reviews_test.index.isin(level_idx)]                            # get the indices of the ones that are in the test set
rnd_idx = sample(list(level_rev.index), min(len(level_rev), 100))                       # sample down to 100 reviews
level_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]  
len(level_rev)

level_base_100 = per_word_att(word='level', sentences=level_test_100, model_origin='bert-base-uncased', model_type=BertModel)
level_pretrained_100 = per_word_att(word='level', sentences=level_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
level_fine_100 = per_word_att(word='level', sentences=level_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(level_base_100, open('attention_values/level_base_100.pkl', 'wb'))
pickle.dump(level_pretrained_100, open('attention_values/level_pretrained_100.pkl', 'wb'))
pickle.dump(level_fine_100, open('attention_values/level_fine_100.pkl', 'wb'))

# Read in retrieved attentions
level_base_100 = pickle.load(open("attention_values/level_base_100.pkl", "rb"))
level_pretrained_100 = pickle.load(open("attention_values/level_pretrained_100.pkl", "rb"))
level_fine_100 = pickle.load(open("attention_values/level_fine_100.pkl", "rb"))

# Find Top 5 of all words LEVEL is attending to

sentences = list(level_base_100.keys())
level_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([level_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    level_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    level_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    level_top = pd.concat([level_top, df], axis=1)
    
level_top = level_top.drop('level', axis='columns').transpose()
level_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(level_top[1]) / np.mean(level_top[0]) - 1, 4)
fine = np.round(np.mean(level_top[2]) / np.mean(level_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

level_top['word'] = level_top.index 
level_top = level_top.groupby(['word']).mean()

level_top.sort_values(ascending=False, by=[0]).head(5)

level_top.sort_values(ascending=False, by=[1]).head(5)

level_top.sort_values(ascending=False, by=[2]).head(5)

### MEXICAN

# get a random 100 reviews from the test set that contain the word 'mexican' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
mexican_idx = preprocessed_text[(preprocessed_text['text length'] <= 400)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('mexican'))].index # ... sometimes 'mexican' is cut off then
mexican_rev = reviews_test[reviews_test.index.isin(mexican_idx)]                            # get the indices of the ones that are in the test set
rnd_idx = sample(list(mexican_rev.index), min(len(mexican_rev), 100))                       # sample down to 100 reviews
mexican_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]  
len(mexican_rev)

mexican_base_100 = per_word_att(word='mexican', sentences=mexican_test_100, model_origin='bert-base-uncased', model_type=BertModel)
mexican_pretrained_100 = per_word_att(word='mexican', sentences=mexican_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
mexican_fine_100 = per_word_att(word='mexican', sentences=mexican_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(mexican_base_100, open('attention_values/mexican_base_100.pkl', 'wb'))
pickle.dump(mexican_pretrained_100, open('attention_values/mexican_pretrained_100.pkl', 'wb'))
pickle.dump(mexican_fine_100, open('attention_values/mexican_fine_100.pkl', 'wb'))

# Read in retrieved attentions
mexican_base_100 = pickle.load(open("attention_values/mexican_base_100.pkl", "rb"))
mexican_pretrained_100 = pickle.load(open("attention_values/mexican_pretrained_100.pkl", "rb"))
mexican_fine_100 = pickle.load(open("attention_values/mexican_fine_100.pkl", "rb"))

#### MEXICAN's attention to all words

# Find Top 5 of all words MEXICAN is attending to

sentences = list(mexican_base_100.keys())
mexican_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([mexican_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    mexican_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    mexican_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    mexican_top = pd.concat([mexican_top, df], axis=1)
    
mexican_top = mexican_top.drop('mexican', axis='columns').transpose()
mexican_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(mexican_top[1]) / np.mean(mexican_top[0]) - 1, 4)
fine = np.round(np.mean(mexican_top[2]) / np.mean(mexican_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

mexican_top['word'] = mexican_top.index 
mexican_top = mexican_top.groupby(['word']).mean()

mexican_top.sort_values(ascending=False, by=[0]).head(5)

mexican_top.sort_values(ascending=False, by=[1]).head(5)

mexican_top.sort_values(ascending=False, by=[2]).head(5)

### US

# get a random 100 reviews from the test set that contain the word 'us' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
us_idx = preprocessed_text[(preprocessed_text['text length'] <= 400)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('us'))].index # ... sometimes 'us' is cut off then
us_rev = reviews_test[reviews_test.index.isin(us_idx)]                            # get the indices of the ones that are in the test set
rnd_idx = sample(list(us_rev.index), min(len(us_rev), 100))                       # sample down to 100 reviews
us_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]  
len(us_rev)

us_base_100 = per_word_att(word='us', sentences=us_test_100, model_origin='bert-base-uncased', model_type=BertModel)
us_pretrained_100 = per_word_att(word='us', sentences=us_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
us_fine_100 = per_word_att(word='us', sentences=us_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(us_base_100, open('attention_values/us_base_100.pkl', 'wb'))
pickle.dump(us_pretrained_100, open('attention_values/us_pretrained_100.pkl', 'wb'))
pickle.dump(us_fine_100, open('attention_values/us_fine_100.pkl', 'wb'))

# Read in retrieved attentions
us_base_100 = pickle.load(open("attention_values/us_base_100.pkl", "rb"))
us_pretrained_100 = pickle.load(open("attention_values/us_pretrained_100.pkl", "rb"))
us_fine_100 = pickle.load(open("attention_values/us_fine_100.pkl", "rb"))

# Find Top 5 of all words US is attending to

sentences = list(us_base_100.keys())
us_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([us_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    us_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    us_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    us_top = pd.concat([us_top, df], axis=1)
    
us_top = us_top.drop('us', axis='columns').transpose()
us_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(us_top[1]) / np.mean(us_top[0]) - 1, 4)
fine = np.round(np.mean(us_top[2]) / np.mean(us_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

us_top['word'] = us_top.index 
us_top = us_top.groupby(['word']).mean()

us_top.sort_values(ascending=False, by=[0]).head(5)

us_top.sort_values(ascending=False, by=[1]).head(5)

us_top.sort_values(ascending=False, by=[2]).head(5)

### Important words from unsupervised learning (Group 3)

### DINER

# get a random 100 reviews from the test set that contain the word 'diner' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
diner_idx = preprocessed_text[(preprocessed_text['text length'] <= 512)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('diner'))].index # ... sometimes keyword is cut off then
diner_rev = reviews_test[reviews_test.index.isin(diner_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(diner_rev.index), min(len(diner_rev), 100))                                      # sample down to 100 reviews
diner_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
len(diner_rev)

# Get attention values for diner for test set sentences from all models
diner_base_100 = per_word_att(word='diner', sentences=diner_test_100, model_origin='bert-base-uncased', model_type=BertModel)
diner_pretrained_100 = per_word_att(word='diner', sentences=diner_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
diner_fine_100 = per_word_att(word='diner', sentences=diner_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(diner_base_100, open('attention_values/diner_base_100.pkl', 'wb'))
pickle.dump(diner_pretrained_100, open('attention_values/diner_pretrained_100.pkl', 'wb'))
pickle.dump(diner_fine_100, open('attention_values/diner_fine_100.pkl', 'wb'))

# Read in retrieved attentions
diner_base_100 = pickle.load(open("attention_values/diner_base_100.pkl", "rb"))
diner_pretrained_100 = pickle.load(open("attention_values/diner_pretrained_100.pkl", "rb"))
diner_fine_100 = pickle.load(open("attention_values/diner_fine_100.pkl", "rb"))

# Get change in attention for DINER for all models

sentences = list(diner_base_100.keys())
diner_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([diner_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    diner_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    diner_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    diner_top = pd.concat([diner_top, df], axis=1)
    
diner_top = diner_top.drop('diner', axis='columns').transpose()
diner_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(diner_top[1]) / np.mean(diner_top[0]) - 1, 4)
fine = np.round(np.mean(diner_top[2]) / np.mean(diner_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

diner_top['word'] = diner_top.index 
diner_top = diner_top.groupby(['word']).mean()

diner_top.sort_values(ascending=False, by=[0]).head(5)

diner_top.sort_values(ascending=False, by=[1]).head(5)

diner_top.sort_values(ascending=False, by=[2]).head(5)

# What is the context in the review containing 'poisoning'? 
sentences = list(diner_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(diner_base_100[sentence].columns))
matches = [match for match in l if " lap " in match] 
print(matches)

### INDIAN

# get a random 100 reviews from the test set that contain the word 'indian' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
indian_idx = preprocessed_text[(preprocessed_text['text length'] <= 512)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('indian'))].index # ... sometimes keyword is cut off then
indian_rev = reviews_test[reviews_test.index.isin(indian_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(indian_rev.index), min(len(indian_rev), 100))                                      # sample down to 100 reviews
indian_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
len(indian_rev)

# Get attention values for indian for test set sentences from all models
indian_base_100 = per_word_att(word='indian', sentences=indian_test_100, model_origin='bert-base-uncased', model_type=BertModel)
indian_pretrained_100 = per_word_att(word='indian', sentences=indian_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
indian_fine_100 = per_word_att(word='indian', sentences=indian_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(indian_base_100, open('attention_values/indian_base_100.pkl', 'wb'))
pickle.dump(indian_pretrained_100, open('attention_values/indian_pretrained_100.pkl', 'wb'))
pickle.dump(indian_fine_100, open('attention_values/indian_fine_100.pkl', 'wb'))

# Read in retrieved attentions
indian_base_100 = pickle.load(open("attention_values/indian_base_100.pkl", "rb"))
indian_pretrained_100 = pickle.load(open("attention_values/indian_pretrained_100.pkl", "rb"))
indian_fine_100 = pickle.load(open("attention_values/indian_fine_100.pkl", "rb"))

# Get change in attention for INDIAN for all models

sentences = list(indian_base_100.keys())
indian_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([indian_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    indian_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    indian_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    indian_top = pd.concat([indian_top, df], axis=1)
    
indian_top = indian_top.drop('indian', axis='columns').transpose()
indian_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(indian_top[1]) / np.mean(indian_top[0]) - 1, 4)
fine = np.round(np.mean(indian_top[2]) / np.mean(indian_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

indian_top['word'] = indian_top.index 
indian_top = indian_top.groupby(['word']).mean()

indian_top.sort_values(ascending=False, by=[0]).head(5)

indian_top.sort_values(ascending=False, by=[1]).head(5)

indian_top.sort_values(ascending=False, by=[2]).head(5)

# What is the context in the review containing 'poisoning'? 
sentences = list(indian_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(indian_base_100[sentence].columns))
matches = [match for match in l if " version " in match] 
print(matches)

### PIZZA

# get a random 100 reviews from the test set that contain the word 'pizza' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
pizza_idx = preprocessed_text[(preprocessed_text['text length'] <= 200)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('pizza'))].index # ... sometimes keyword is cut off then
pizza_rev = reviews_test[reviews_test.index.isin(pizza_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(pizza_rev.index), min(len(pizza_rev), 100))                                      # sample down to 100 reviews
pizza_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
len(pizza_rev)

# Get attention values for pizza for test set sentences from all models
pizza_base_100 = per_word_att(word='pizza', sentences=pizza_test_100, model_origin='bert-base-uncased', model_type=BertModel)
pizza_pretrained_100 = per_word_att(word='pizza', sentences=pizza_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
pizza_fine_100 = per_word_att(word='pizza', sentences=pizza_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(pizza_base_100, open('attention_values/pizza_base_100.pkl', 'wb'))
pickle.dump(pizza_pretrained_100, open('attention_values/pizza_pretrained_100.pkl', 'wb'))
pickle.dump(pizza_fine_100, open('attention_values/pizza_fine_100.pkl', 'wb'))

# Read in retrieved attentions
pizza_base_100 = pickle.load(open("attention_values/pizza_base_100.pkl", "rb"))
pizza_pretrained_100 = pickle.load(open("attention_values/pizza_pretrained_100.pkl", "rb"))
pizza_fine_100 = pickle.load(open("attention_values/pizza_fine_100.pkl", "rb"))

# Find the difference in attention for PIZZA between the models

sentences = list(pizza_base_100.keys())
pizza_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([pizza_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    pizza_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    pizza_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    pizza_top = pd.concat([pizza_top, df], axis=1)
    
pizza_top = pizza_top.drop('pizza', axis='columns').transpose()
pizza_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(pizza_top[1]) / np.mean(pizza_top[0]) - 1, 4)
fine = np.round(np.mean(pizza_top[2]) / np.mean(pizza_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

pizza_top['word'] = pizza_top.index 
pizza_top = pizza_top.groupby(['word']).mean()

pizza_top.sort_values(ascending=False, by=[0]).head(5)

pizza_top.sort_values(ascending=False, by=[1]).head(5)

pizza_top.sort_values(ascending=False, by=[2]).head(5)

# What is the context in the review containing 'poisoning'? 
sentences = list(pizza_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(pizza_base_100[sentence].columns))
matches = [match for match in l if " planet " in match] 
print(matches)

### Unimportant words from Unsupervised (Group 4)

### FOOD

## see in group 1: unimportant words from supervised learning

### NAMES

# get a random 100 reviews from the test set that contain the word 'names' AND are shorter than 100 words
random.seed(42)
# get indices of all described reviews
names_idx = preprocessed_text[(preprocessed_text['text length'] <= 512)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('names'))].index # ... sometimes keyword is cut off then
names_rev = reviews_test[reviews_test.index.isin(names_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(names_rev.index), min(len(names_rev), 100))                                      # sample down to 100 reviews
names_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
len(names_rev)

# Get attention values for names for test set sentences from all models
names_base_100 = per_word_att(word='names', sentences=names_test_100, model_origin='bert-base-uncased', model_type=BertModel)
names_pretrained_100 = per_word_att(word='names', sentences=names_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
names_fine_100 = per_word_att(word='names', sentences=names_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(names_base_100, open('attention_values/names_base_100.pkl', 'wb'))
pickle.dump(names_pretrained_100, open('attention_values/names_pretrained_100.pkl', 'wb'))
pickle.dump(names_fine_100, open('attention_values/names_fine_100.pkl', 'wb'))

# Read in retrieved attentions
names_base_100 = pickle.load(open("attention_values/names_base_100.pkl", "rb"))
names_pretrained_100 = pickle.load(open("attention_values/names_pretrained_100.pkl", "rb"))
names_fine_100 = pickle.load(open("attention_values/names_fine_100.pkl", "rb"))

# Get change in attention for NAMES for all models

sentences = list(names_base_100.keys())
names_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([names_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    names_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    names_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    names_top = pd.concat([names_top, df], axis=1)
    
names_top = names_top.drop('names', axis='columns').transpose()
names_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(names_top[1]) / np.mean(names_top[0]) - 1, 4)
fine = np.round(np.mean(names_top[2]) / np.mean(names_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

names_top['word'] = names_top.index 
names_top = names_top.groupby(['word']).mean()

names_top.sort_values(ascending=False, by=[0]).head(5)

names_top.sort_values(ascending=False, by=[1]).head(5)

names_top.sort_values(ascending=False, by=[2]).head(5)

# What is the context in the review containing 'poisoning'? 
sentences = list(would_base_100.keys())
l =[]
for sentence in sentences:
    l.append(' '.join(would_base_100[sentence].columns))
matches = [match for match in l if " lap " in match] 
print(matches)

### WOULD

# get a random 100 reviews from the test set that contain the word 'would' AND are shorter than 150 words
random.seed(42)
# get indices of all described reviews
would_idx = preprocessed_text[(preprocessed_text['text length'] <= 170)               # is needed becuase samples are truncated...
                              & (preprocessed_text.text.str.contains('would'))].index # ... sometimes keyword is cut off then
would_rev = reviews_test[reviews_test.index.isin(would_idx)]                          # get the indices of the ones that are in the test set
rnd_idx = sample(list(would_rev.index), min(len(would_rev), 100))                                      # sample down to 100 reviews
would_test_100 = reviews_test[reviews_test.index.isin(rnd_idx)]                       # get these sample reviews
len(would_rev)

# Get attention values for would for test set sentences from all models
would_base_100 = per_word_att(word='would', sentences=would_test_100, model_origin='bert-base-uncased', model_type=BertModel)
would_pretrained_100 = per_word_att(word='would', sentences=would_test_100, model_origin='output_pretrained/', model_type=BertForMaskedLM)
would_fine_100 = per_word_att(word='would', sentences=would_test_100, model_origin='output_finetuned/', model_type=BertForSequenceClassification)

# Save the attentions values as pickle files for not re-doing the retrieval
pickle.dump(would_base_100, open('attention_values/would_base_100.pkl', 'wb'))
pickle.dump(would_pretrained_100, open('attention_values/would_pretrained_100.pkl', 'wb'))
pickle.dump(would_fine_100, open('attention_values/would_fine_100.pkl', 'wb'))

# Read in retrieved attentions
would_base_100 = pickle.load(open("attention_values/would_base_100.pkl", "rb"))
would_pretrained_100 = pickle.load(open("attention_values/would_pretrained_100.pkl", "rb"))
would_fine_100 = pickle.load(open("attention_values/would_fine_100.pkl", "rb"))

# Get change in attention for WOULD for all models

sentences = list(would_base_100.keys())
would_top = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([would_base_100[sentence].iloc[:,1:-3].sum(axis=0),            # Base model is the first row  
                    would_pretrained_100[sentence].iloc[:,1:-3].sum(axis=0),      # Pretrained model is the second row
                    would_fine_100[sentence].iloc[:,1:-3].sum(axis=0)],           # Finetuned model is the third row
                   axis=1).transpose()
    
    would_top = pd.concat([would_top, df], axis=1)
    
would_top = would_top.drop('would', axis='columns').transpose()
would_top.describe()

# relative change due to training style compared to Base model 
pre = np.round(np.mean(would_top[1]) / np.mean(would_top[0]) - 1, 4)
fine = np.round(np.mean(would_top[2]) / np.mean(would_top[0]) - 1, 4)
print('Relative change for pretraining: {0} \nRelative change for fine-tuning: {1}'.format(pre, fine))

##### Top 5 words

would_top['word'] = would_top.index 
would_top = would_top.groupby(['word']).mean()

would_top.sort_values(ascending=False, by=[0]).head(5)

would_top.sort_values(ascending=False, by=[1]).head(5)

would_top.sort_values(ascending=False, by=[2]).head(5)

## Layer-wise attention values analysis

### GREAT: Change in Layers

# Find out layer-wise attention for GREAT

sentences = list(great_base_100.keys())
great_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([great_base_100[sentence].iloc[:,1:-3].drop('great', axis='columns').sum(axis=1),            # Base model is the first row  
                    great_pretrained_100[sentence].iloc[:,1:-3].drop('great', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    great_fine_100[sentence].iloc[:,1:-3].drop('great', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    great_layer = pd.concat([great_layer, df], axis=0)

great_layer['layer'] = great_layer.index 
great_layer = great_layer.groupby(['layer']).sum()
great_layer

### DELICIOUS: Change in Layers

# Find out layer-wise attention for DELICIOUS

sentences = list(delicious_base_100.keys())
delicious_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([delicious_base_100[sentence].iloc[:,1:-3].drop('delicious', axis='columns').sum(axis=1),            # Base model is the first row  
                    delicious_pretrained_100[sentence].iloc[:,1:-3].drop('delicious', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    delicious_fine_100[sentence].iloc[:,1:-3].drop('delicious', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    delicious_layer = pd.concat([delicious_layer, df], axis=0)

delicious_layer['layer'] = delicious_layer.index 
delicious_layer = delicious_layer.groupby(['layer']).sum()
delicious_layer

### GOOD: Change in Layers

# Find out layer-wise attention for GOOD

sentences = list(good_base_100.keys())
good_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([good_base_100[sentence].iloc[:,1:-3].drop('good', axis='columns').sum(axis=1),            # Base model is the first row  
                    good_pretrained_100[sentence].iloc[:,1:-3].drop('good', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    good_fine_100[sentence].iloc[:,1:-3].drop('good', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    good_layer = pd.concat([good_layer, df], axis=0)

good_layer['layer'] = good_layer.index 
good_layer = good_layer.groupby(['layer']).sum()
good_layer

### FOOD: Change in Layers

# Find out layer-wise attention for FOOD

sentences = list(food_base_100.keys())
food_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([food_base_100[sentence].iloc[:,1:-3].drop('food', axis='columns').sum(axis=1),            # Base model is the first row  
                    food_pretrained_100[sentence].iloc[:,1:-3].drop('food', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    food_fine_100[sentence].iloc[:,1:-3].drop('food', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    food_layer = pd.concat([food_layer, df], axis=0)

food_layer['layer'] = food_layer.index 
food_layer = food_layer.groupby(['layer']).sum()
food_layer

### LEVEL: Change in Layers

# Find out layer-wise attention for LEVEL

sentences = list(level_base_100.keys())
level_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([level_base_100[sentence].iloc[:,1:-3].drop('level', axis='columns').sum(axis=1),            # Base model is the first row  
                    level_pretrained_100[sentence].iloc[:,1:-3].drop('level', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    level_fine_100[sentence].iloc[:,1:-3].drop('level', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    level_layer = pd.concat([level_layer, df], axis=0)

level_layer['layer'] = level_layer.index 
level_layer = level_layer.groupby(['layer']).sum()
level_layer

### MEXICAN: Change in Layers

# Find out layer-wise attention for MEXICAN

sentences = list(mexican_base_100.keys())
mexican_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([mexican_base_100[sentence].iloc[:,1:-3].drop('mexican', axis='columns').sum(axis=1),            # Base model is the first row  
                    mexican_pretrained_100[sentence].iloc[:,1:-3].drop('mexican', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    mexican_fine_100[sentence].iloc[:,1:-3].drop('mexican', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    mexican_layer = pd.concat([mexican_layer, df], axis=0)

mexican_layer['layer'] = mexican_layer.index 
mexican_layer = mexican_layer.groupby(['layer']).sum()
mexican_layer

### US: Change in Layers

# Find out layer-wise attention for US

sentences = list(us_base_100.keys())
us_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([us_base_100[sentence].iloc[:,1:-3].drop('us', axis='columns').sum(axis=1),            # Base model is the first row  
                    us_pretrained_100[sentence].iloc[:,1:-3].drop('us', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    us_fine_100[sentence].iloc[:,1:-3].drop('us', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    us_layer = pd.concat([us_layer, df], axis=0)

us_layer['layer'] = us_layer.index 
us_layer = us_layer.groupby(['layer']).sum()
us_layer

### DINER: Change in Layers

# Find out layer-wise attention for DINER

sentences = list(diner_base_100.keys())
diner_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([diner_base_100[sentence].iloc[:,1:-3].drop('diner', axis='columns').sum(axis=1),            # Base model is the first row  
                    diner_pretrained_100[sentence].iloc[:,1:-3].drop('diner', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    diner_fine_100[sentence].iloc[:,1:-3].drop('diner', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    diner_layer = pd.concat([diner_layer, df], axis=0)

diner_layer['layer'] = diner_layer.index 
diner_layer = diner_layer.groupby(['layer']).sum()
diner_layer

### INDIAN: Change in Layers

# Find out layer-wise attention for INDIAN

sentences = list(indian_base_100.keys())
indian_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([indian_base_100[sentence].iloc[:,1:-3].drop('indian', axis='columns').sum(axis=1),            # Base model is the first row  
                    indian_pretrained_100[sentence].iloc[:,1:-3].drop('indian', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    indian_fine_100[sentence].iloc[:,1:-3].drop('indian', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    indian_layer = pd.concat([indian_layer, df], axis=0)

indian_layer['layer'] = indian_layer.index 
indian_layer = indian_layer.groupby(['layer']).sum()
indian_layer

### PIZZA: Change in Layers

# Find out layer-wise attention for PIZZA

sentences = list(pizza_base_100.keys())
pizza_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([pizza_base_100[sentence].iloc[:,1:-3].drop('pizza', axis='columns').sum(axis=1),            # Base model is the first row  
                    pizza_pretrained_100[sentence].iloc[:,1:-3].drop('pizza', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    pizza_fine_100[sentence].iloc[:,1:-3].drop('pizza', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    pizza_layer = pd.concat([pizza_layer, df], axis=0)

pizza_layer['layer'] = pizza_layer.index 
pizza_layer = pizza_layer.groupby(['layer']).sum()
pizza_layer

### FOOD: Change in Layers

# see above

### NAMES: Change in Layers

# Find out layer-wise attention for NAMES

sentences = list(names_base_100.keys())
names_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([names_base_100[sentence].iloc[:,1:-3].drop('names', axis='columns').sum(axis=1),            # Base model is the first row  
                    names_pretrained_100[sentence].iloc[:,1:-3].drop('names', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    names_fine_100[sentence].iloc[:,1:-3].drop('names', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    names_layer = pd.concat([names_layer, df], axis=0)

names_layer['layer'] = names_layer.index 
names_layer = names_layer.groupby(['layer']).sum()
names_layer

### WOULD: Change in Layers

# Find out layer-wise attention

sentences = list(would_base_100.keys())
would_layer = pd.DataFrame()
for sentence in sentences:
      
    # concatenate the sum of attentions for each word in each model
    df = pd.concat([would_base_100[sentence].iloc[:,1:-3].drop('would', axis='columns').sum(axis=1),            # Base model is the first row  
                    would_pretrained_100[sentence].iloc[:,1:-3].drop('would', axis='columns').sum(axis=1),      # Pretrained model is the second row
                    would_fine_100[sentence].iloc[:,1:-3].drop('would', axis='columns').sum(axis=1)],           # Finetuned model is the third row
                   axis=1)
    would_layer = pd.concat([would_layer, df], axis=0)

would_layer['layer'] = would_layer.index 
would_layer = would_layer.groupby(['layer']).sum()
would_layer

### Aggregating per word group

# Group 1: important words from supervised learning
important_layers = pd.concat([great_layer, delicious_layer, good_layer], axis=0).groupby(['layer']).mean()
important_layers['rel. change Base -> pretrained'] = np.round(important_layers[1] / important_layers[0] - 1, 3)
important_layers['rel. change Base -> Finetuned'] = np.round(important_layers[2] / important_layers[0] - 1, 3)
important_layers

# Group 2: unimportant words from supervised learning
unimportant_layers = pd.concat([food_layer, mexican_layer, us_layer, level_layer], axis=0).groupby(['layer']).mean()
unimportant_layers['rel. change Base -> pretrained'] = np.round(unimportant_layers[1] / unimportant_layers[0] - 1, 3)
unimportant_layers['rel. change Base -> Finetuned'] = np.round(unimportant_layers[2] / unimportant_layers[0] - 1, 3)
unimportant_layers

# Group 3: important words from unsupervised learning
important_layers = pd.concat([diner_layer, pizza_layer, indian_layer], axis=0).groupby(['layer']).mean()
important_layers['rel. change Base -> pretrained'] = np.round(important_layers[1] / important_layers[0] - 1, 3)
important_layers['rel. change Base -> Finetuned'] = np.round(important_layers[2] / important_layers[0] - 1, 3)
important_layers

# Group 4: unimportant words from unsupervised learning
unimportant_layers = pd.concat([food_layer, names_layer, would_layer], axis=0).groupby(['layer']).mean()
unimportant_layers['rel. change Base -> pretrained'] = np.round(unimportant_layers[1] / unimportant_layers[0] - 1, 3)
unimportant_layers['rel. change Base -> Finetuned'] = np.round(unimportant_layers[2] / unimportant_layers[0] - 1, 3)
unimportant_layers

### Aggregating per learning style

# Supervised learning (Group 1 and Group 2)
ovr_layers = pd.concat([great_layer, delicious_layer, good_layer, food_layer, mexican_layer, us_layer, level_layer], axis=0).groupby(['layer']).mean()
ovr_layers['rel. change Base -> pretrained'] = np.round(ovr_layers[1] / ovr_layers[0] - 1, 3)
ovr_layers['rel. change Base -> Finetuned'] = np.round(ovr_layers[2] / ovr_layers[0] - 1, 3)
ovr_layers

# Unsupervised learning (Group 3 and Group 4)
unsuper_ovr_layers = pd.concat([diner_layer, pizza_layer, indian_layer, food_layer, names_layer, would_layer], axis=0).groupby(['layer']).mean()
unsuper_ovr_layers['rel. change Base -> pretrained'] = np.round(unsuper_ovr_layers[1] / unsuper_ovr_layers[0] - 1, 3)
unsuper_ovr_layers['rel. change Base -> Finetuned'] = np.round(unsuper_ovr_layers[2] / unsuper_ovr_layers[0] - 1, 3)
unsuper_ovr_layers

### Overall aggregation

# Get the average for all words inspected from both learning styles
super_unsuper = pd.concat([unsuper_ovr_layers, ovr_layers], axis=0).groupby(['layer']).mean()
super_unsuper['rel. change Base -> pretrained'] = np.round(super_unsuper[1] / super_unsuper[0] - 1, 3)
super_unsuper['rel. change Base -> Finetuned'] = np.round(super_unsuper[2] / super_unsuper[0] - 1, 3)
super_unsuper
