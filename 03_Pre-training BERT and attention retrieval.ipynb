{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from argparse import Namespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed Data\n",
    "preprocessed_text = pickle.load(open(\"data/preprocessed_text.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fixed train and test split\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(preprocessed_text.text, preprocessed_text.classes,\n",
    "                                                                test_size = 0.2, random_state = 42,\n",
    "                                                                stratify=preprocessed_text.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the reviews into two partitions for training to avoid timeout\n",
    "\n",
    "#reviews_train = reviews_train[0:24000]\n",
    "reviews_train = reviews_train[24000:48000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train set to a .txt file\n",
    "np.savetxt(fname='data/train.txt', X=np.array(reviews_train.values.tolist()), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tokenizer and BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/braincourt.de/twalenczak/.local/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  24000\n",
      "CPU times: user 50.2 s, sys: 215 ms, total: 50.4 s\n",
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "transformers has a predefined class LineByLineTextDataset()\n",
    "which reads your text line by line and converts them to tokens\n",
    "'''\n",
    "\n",
    "dataset= LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = 'data/train.txt',\n",
    "    block_size = 512    # maximum sequence length\n",
    ")\n",
    "\n",
    "print('No. of lines: ', len(dataset)) # No of lines in your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  109514298\n"
     ]
    }
   ],
   "source": [
    "#Use the pre-trained weights instead of custom config like in the source\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('output_pretrained/',\n",
    "                                        output_attentions = True)      # Whether the model returns attentions weights.\n",
    "\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output_pretrained/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=1e-4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 14:31:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.311600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3d 20h 44min 50s, sys: 3h 18min, total: 4d 2min 50s\n",
      "Wall time: 14h 31min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=4.432013997395833, metrics={'train_runtime': 52314.9608, 'train_samples_per_second': 1.376, 'train_steps_per_second': 0.043, 'total_flos': 7118898967309824.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('output_pretrained/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check performance compared to base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5289820432662964,\n",
       "  'sequence': 'this italian restaurant had the best food i have ever eaten.',\n",
       "  'token': 2833,\n",
       "  'token_str': 'f o o d'},\n",
       " {'score': 0.048788491636514664,\n",
       "  'sequence': 'this italian restaurant had the best pizza i have ever eaten.',\n",
       "  'token': 10733,\n",
       "  'token_str': 'p i z z a'},\n",
       " {'score': 0.04705256223678589,\n",
       "  'sequence': 'this italian restaurant had the best meal i have ever eaten.',\n",
       "  'token': 7954,\n",
       "  'token_str': 'm e a l'},\n",
       " {'score': 0.0386321023106575,\n",
       "  'sequence': 'this italian restaurant had the best menu i have ever eaten.',\n",
       "  'token': 12183,\n",
       "  'token_str': 'm e n u'},\n",
       " {'score': 0.032607030123472214,\n",
       "  'sequence': 'this italian restaurant had the best dinner i have ever eaten.',\n",
       "  'token': 4596,\n",
       "  'token_str': 'd i n n e r'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PoC\n",
    "#Load both models and pipeline for comparison\n",
    "\n",
    "#Base BERT \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True)\n",
    "base_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "fill_mask2 = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fill_mask2('This italian restaurant had the best [MASK] I have ever eaten.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2478836625814438,\n",
       "  'sequence': 'this italian restaurant had the best food i have ever eaten.',\n",
       "  'token': 2833,\n",
       "  'token_str': 'f o o d'},\n",
       " {'score': 0.09602867066860199,\n",
       "  'sequence': 'this italian restaurant had the best service i have ever eaten.',\n",
       "  'token': 2326,\n",
       "  'token_str': 's e r v i c e'},\n",
       " {'score': 0.08484292775392532,\n",
       "  'sequence': 'this italian restaurant had the best pizza i have ever eaten.',\n",
       "  'token': 10733,\n",
       "  'token_str': 'p i z z a'},\n",
       " {'score': 0.056329403072595596,\n",
       "  'sequence': 'this italian restaurant had the best experience i have ever eaten.',\n",
       "  'token': 3325,\n",
       "  'token_str': 'e x p e r i e n c e'},\n",
       " {'score': 0.0265803299844265,\n",
       "  'sequence': 'this italian restaurant had the best meal i have ever eaten.',\n",
       "  'token': 7954,\n",
       "  'token_str': 'm e a l'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrained BERT\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True)\n",
    "\n",
    "# Load specific model\n",
    "model_pretrained = BertForMaskedLM.from_pretrained('output_pretrained/')\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_pretrained,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fill_mask('This italian restaurant had the best [MASK] I have ever eaten.')\n",
    "\n",
    "# Service and Pizza gained much higher score after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance of pre-trained model to base model by randomly masking words in test set\n",
    "replaced_words = []\n",
    "sentences = []\n",
    "\n",
    "for sent in reviews_test:\n",
    "    words=sent.split(\" \")\n",
    "    n = np.random.randint(low=0, high=len(words))\n",
    "    replaced_words.append(words[n])\n",
    "    words[n]=\"[MASK]\"\n",
    "    masked_sentence = \" \".join(words)\n",
    "    sentences.append(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hip place said food someone whose tastes buds ...</td>\n",
       "      <td>atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ordered place ubereats decided give place try ...</td>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hold phone someone say cent cupcakes hold need...</td>\n",
       "      <td>cupcakes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probably couple hundred times last years remem...</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekend getaway whistler friends stopped [MASK...</td>\n",
       "      <td>brunch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence        word\n",
       "0  hip place said food someone whose tastes buds ...  atmosphere\n",
       "1  ordered place ubereats decided give place try ...      things\n",
       "2  hold phone someone say cent cupcakes hold need...    cupcakes\n",
       "3  probably couple hundred times last years remem...        care\n",
       "4  weekend getaway whistler friends stopped [MASK...      brunch"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "df['word'] = replaced_words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with base.\n",
      "The base model's accuracy on the test set is: 0.0496\n",
      "\n",
      "Done with pre-trained.\n",
      "The pre-trained model's accuracy on the test set is: 0.161\n"
     ]
    }
   ],
   "source": [
    "pred2 = [fill_mask2(sent, top_k=1)[0]['token_str'].replace(\" \",\"\") for sent in df.sentence.to_list()]\n",
    "print('Done with base.')\n",
    "acc2 = accuracy_score(df.word.to_list(), pred2)\n",
    "print(\"The base model's accuracy on the test set is: {0}\" .format(acc2))\n",
    "\n",
    "pred = [fill_mask(sent, top_k=1)[0]['token_str'].replace(\" \",\"\") for sent in df.sentence.to_list()]\n",
    "print('\\nDone with pre-trained.')\n",
    "acc = accuracy_score(df.word.to_list(), pred)\n",
    "print(\"The pre-trained model's accuracy on the test set is: {0}\" .format(acc))\n",
    "\n",
    "#The base model's accuracy on the test set is: 0.0496\n",
    "#The pre-trained model's accuracy on the test set is: 0.161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve attention values on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve attention values for each class of the test set and concatenate them into a dictionary\n",
    "\n",
    "# Count the number of words in the longest review and save it to max_length\n",
    "wordcountList = [len(re.sub(\"[^\\w]\", \" \",  review).split()) for review in reviews_test]\n",
    "max_length = max(wordcountList)\n",
    "\n",
    "test_df = pd.concat([reviews_test, y_test], axis=1)\n",
    "pretrained_dict = {}\n",
    "for cond in ['positive', 'neutral', 'negative']:\n",
    "    \n",
    "    # slice the data \n",
    "    test_cond = test_df[test_df[\"classes\"] == cond]\n",
    "    \n",
    "    # tokenize the text and retrieve the attentions\n",
    "    inputs = tokenizer.encode_plus(test_cond.text.to_list(),         # Sentence to encode.\n",
    "                                   add_special_tokens = True,        # Add '[CLS]' and '[SEP]'\n",
    "                                   padding = 'longest',              # Pad & truncate all sentences.\n",
    "                                   max_length = max_length,          # Define sequence length as longest sequence of test set\n",
    "                                   truncation = True,                # truncate sample if too long\n",
    "                                   pad_to_max_length = True,         # add padding tokens if shorter sequence\n",
    "                                   return_attention_mask = True,     # Construct attn. masks.\n",
    "                                   return_tensors = 'pt')            # return pytorch tensors\n",
    "    outputs = model_pretrained(**inputs, output_attentions=True)\n",
    "    attentions_pretrained_cond = outputs.attentions\n",
    "    \n",
    "    # add it to the dictionary\n",
    "    pretrained_dict[\"{0}\".format(cond)] = attentions_pretrained_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 : torch.Size([1, 12, 148, 148])\n",
      "Layer 2 : torch.Size([1, 12, 148, 148])\n",
      "Layer 3 : torch.Size([1, 12, 148, 148])\n",
      "Layer 4 : torch.Size([1, 12, 148, 148])\n",
      "Layer 5 : torch.Size([1, 12, 148, 148])\n",
      "Layer 6 : torch.Size([1, 12, 148, 148])\n",
      "Layer 7 : torch.Size([1, 12, 148, 148])\n",
      "Layer 8 : torch.Size([1, 12, 148, 148])\n",
      "Layer 9 : torch.Size([1, 12, 148, 148])\n",
      "Layer 10 : torch.Size([1, 12, 148, 148])\n",
      "Layer 11 : torch.Size([1, 12, 148, 148])\n",
      "Layer 12 : torch.Size([1, 12, 148, 148])\n"
     ]
    }
   ],
   "source": [
    "# Show shape of each attention layer tensor\n",
    "for i in range(12):\n",
    "    print(\"Layer\",i+1,\":\",pretrained_dict['negative'][i].size())\n",
    "# [batch_size, num_heads, sequence_length, sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attention values to pkl file\n",
    "pickle.dump(pretrained_dict, open('attention_values/attentions_pretrained.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Bert base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Retrieve attention values for each class of the test set and concatenate them into a dictionary\n",
    "\n",
    "# Count the number of words in the longest review and save it to max_length\n",
    "wordcountList = [len(re.sub(\"[^\\w]\", \" \",  review).split()) for review in reviews_test]\n",
    "max_length = max(wordcountList)\n",
    "\n",
    "# Load specific model\n",
    "Bertbase = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test_df = pd.concat([reviews_test, y_test], axis=1)\n",
    "base_dict = {}\n",
    "for cond in ['positive', 'neutral', 'negative']:\n",
    "    \n",
    "    # slice the data \n",
    "    test_cond = test_df[test_df[\"classes\"] == cond]\n",
    "    \n",
    "    # tokenize the text and retrieve the attentions\n",
    "    inputs = tokenizer.encode_plus(test_cond.text.to_list(),         # Sentence to encode.\n",
    "                                   add_special_tokens = True,        # Add '[CLS]' and '[SEP]'\n",
    "                                   padding = 'longest',              # Pad & truncate all sentences.\n",
    "                                   truncation = True,                # truncate sample if too long\n",
    "                                   max_length = max_length,          # Define sequence length as longest sequence of test set\n",
    "                                   pad_to_max_length = True,         # add padding tokens if shorter sequence\n",
    "                                   return_attention_mask = True,     # Construct attn. masks.\n",
    "                                   return_tensors = 'pt')            # return pytorch tensors\n",
    "    outputs = Bertbase(**inputs, output_attentions=True)\n",
    "    attentions_base_cond = outputs.attentions\n",
    "    \n",
    "    # add it to the dictionary\n",
    "    base_dict[\"{0}\".format(cond)] = attentions_base_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 : torch.Size([1, 12, 482, 482])\n",
      "Layer 2 : torch.Size([1, 12, 482, 482])\n",
      "Layer 3 : torch.Size([1, 12, 482, 482])\n",
      "Layer 4 : torch.Size([1, 12, 482, 482])\n",
      "Layer 5 : torch.Size([1, 12, 482, 482])\n",
      "Layer 6 : torch.Size([1, 12, 482, 482])\n",
      "Layer 7 : torch.Size([1, 12, 482, 482])\n",
      "Layer 8 : torch.Size([1, 12, 482, 482])\n",
      "Layer 9 : torch.Size([1, 12, 482, 482])\n",
      "Layer 10 : torch.Size([1, 12, 482, 482])\n",
      "Layer 11 : torch.Size([1, 12, 482, 482])\n",
      "Layer 12 : torch.Size([1, 12, 482, 482])\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(\"Layer\",i+1,\":\",base_dict['negative'][i].size())\n",
    "# [batch_size, num_heads, sequence_length, sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attention values to pkl file\n",
    "pickle.dump(base_dict, open('attention_values/attentions_Bertbase.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
